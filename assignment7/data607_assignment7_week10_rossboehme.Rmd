---
title: "Assignment7-Week10-SentimentAnalysis"
author: "Ross Boehme"
date: "2023-04-02"
output:
  html_document: default
  pdf_document: default
---

## Assignment 7
- In Text Mining with R, [Chapter 2 looks at Sentiment Analysis](https://www.tidytextmining.com/sentiment.html). In this assignment, you should start by getting the primary example code from chapter 2 working in an R Markdown document. You’re then asked to extend the code in two ways:
1. Work with a different corpus of your choosing
2. Incorporate at least one additional sentiment lexicon (possibly from another R package that you’ve found through research).

### Initial Imports and Citations
```{r}
library(tidytext)
library(janeaustenr)
library(tidyverse)
library(textdata)
```
Please note the base code for this project was taken from "Text Mining with R: A Tidy Approach" by Julia Silge and David Robinson. Sebastopol, CA: O’Reilly Media, 2017. ISBN 978-1-491-98165-8. Code below in named chunks "silge-robinson" and "silge-robinson2".

This first R chunk code shows how to "mine" words from Jane Austen's bibliography, with each observation in the dataframe a unique Book, Chapter, Line Number, and Word combination.

```{r silge-robinson}
tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)
```

This second R chunk shows how to load three sentiment lexicons: afinn, bing, and NRC which I'd also like to cite (linked below).
- [afinn](https://search.r-project.org/CRAN/refmans/textdata/html/lexicon_afinn.html)
- [bing](https://search.r-project.org/CRAN/refmans/textdata/html/lexicon_bing.html)
- [nrc](https://cran.r-project.org/web/packages/textdata/textdata.pdf)

```{r silge-robinson2}
afinn <- get_sentiments('afinn')
bing <- get_sentiments('bing')
nrc <- get_sentiments('nrc')
```

I will use this base as inspiration for my own analysis. 

### Extending Base Code With Same Corpus

I will look at a the full available Jane Austen corpus to extend this code, evaluating which of the author's books have the highest percentage of neutral, positive, and negative words.

Showing word count by book. Sense & Sensibility is the longest book. Persuasion is the shortest.
```{r}
austen_word_counts <- as.data.frame(table(tidy_books$book))
colnames(austen_word_counts) <- c("book","total_word_count")
austen_word_counts
```

Taking positive and negative word lexicon from nrc which I'll use to evaluate Jane Austen's corpus. Merging with tidy_books df created previously, then showing positive and negative word count by book.
```{r}
nrc_pos_neg <- nrc %>%
  filter(sentiment == 'negative'| sentiment=='positive')

austen_sentiment <- tidy_books %>%
  inner_join(nrc_pos_neg) %>%
  group_by(book,sentiment) %>% 
  summarise(total_count=n(),.groups = 'drop') %>% 
  as.data.frame() %>%
  pivot_wider(names_from = sentiment,
              values_from = total_count)

colnames(austen_sentiment) <- c("book","negative_words","positive_words")

austen_sentiment
```
Merging total word count df with positive and negative word count df to assess % of all words which were positive or negative by book.
```{r}
austen_combined <- austen_word_counts %>%
  inner_join(austen_sentiment) %>%
  transform(perc_neg = scales::percent(negative_words / total_word_count),
            perc_pos = scales::percent(positive_words / total_word_count),
            neutral_words = total_word_count - negative_words - positive_words)

austen_combined <- austen_combined %>%
  transform(perc_neutr = scales::percent(neutral_words / total_word_count))

austen_final <- austen_combined[,c('book','total_word_count','positive_words','negative_words','neutral_words','perc_pos','perc_neg','perc_neutr')]

austen_final
```

The book with the highest percentage of positive words was: Persuasion. It also had the lowest percentage of negative words.
```{r}
austen_final %>% arrange(desc(perc_pos))
```

The book with the highest percentage of negative words was "Sense & Sensibility" which is interesting because it also had the second highest percentage of positive words. It therefore had fewer "neutral" words, making it a more emotionally charged book than the others.
```{r}
austen_final %>% arrange(desc(perc_neg))
```

### Additional Sentiment Analysis

Per the assignment description, I will now perform an additional sentiment analysis using a different corpus and different lexicon than previously mentioned.

For the lexicon I will use SentimentAnalysis.[R package description linked here](https://cran.r-project.org/web/packages/SentimentAnalysis/vignettes/SentimentAnalysis.html).

For the corpus I will use the ["friends" package](https://cran.rstudio.com/web/packages/friends/index.html) from R where each observation is a piece of speech said by a character in the TV show Friends.

```{r}
library(SentimentAnalysis)
library(friends)

friends_lines <- friends
```

#### Data Cleaning and Exploratory Data Analysis

As the value count generated below shows, there are 699 characters who have speaking lines over the course of Friends. To ensure an adequate sample size, I'll only look at the six main characters who account for the vast majority of lines: Monica Geller, Rachel Green, Ross Geller, Chandler Bing, Joey Tribbiani, and Phoebe Buffay.

Per this dataframe, the characters with the most lines are: Rachel Green (9312), Ross Geller (9157), and Chandler Bing (8465). 
```{r}
length(table(friends_lines$speaker)) 
talkers <- as.data.frame(table(friends_lines$speaker))
top_10_talkers <- talkers %>%
  slice_max(order_by = Freq, n = 10)
names(top_10_talkers) <- c('character','speaking_lines')
top_10_talkers
```
Only top 6 characters in terms of lines spoken.
```{r}
friends_lines <- friends_lines %>% 
  filter(speaker %in% c("Monica Geller", "Rachel Green", "Ross Geller", "Chandler Bing", "Joey Tribbiani", "Phoebe Buffay")) 
```
  
Splitting each row into multiple rows, where each word is its own row (to prepare for sentiment analysis). Delimeter between words is space " ".
```{r}
friends_words <- friends_lines %>% 
  separate_rows(text, sep = " ")
```

Showing which friends characters spoke the most, in terms of lines, words, and words per line. Monica has the fewest words per line (9.8) while Phoebe has the most (10.9). Rachel and Ross have the most lines overall (9312 and 9157) which drives their top total word count (97,633 and 95,561) among all characters. 
```{r}
friends_word_count <- friends_words %>%
  group_by(speaker) %>%
  summarise(total_word_count=n(),
            .groups = 'drop')

top_talkers_lines <- top_10_talkers %>% 
  filter(character %in% c("Monica Geller", "Rachel Green", "Ross Geller", "Chandler Bing", "Joey Tribbiani", "Phoebe Buffay")) 

friends_summary <- left_join(friends_word_count, top_talkers_lines, by=c('speaker'='character'))

friends_summary <- friends_summary %>%
  transform(words_per_line = round((total_word_count / speaking_lines),2))

friends_summary
```

#### Sentiment Analysis

I first attempted to perform sentiment analysis using the analyzeSentiment function in the SentimentAnalysis package. However that function is too slow to work well on individual words. EG friends_words$sentiment <- analyzeSentiment(friends_words$text). It's designed for smaller samples (e.g. a few paragraphs).

Therefore I'll change the SentimentAnalysis' word dictionary ("DictionaryGI") to a dataframe, and perform an analysis by joining the word dataframe with the sentiment dataframe, as we did with the Jane Austen data.

Preparing sentiment df.
```{r}
data(DictionaryGI)
str(DictionaryGI)
length(DictionaryGI$positive) <- length(DictionaryGI$negative)

sa_df <- as.data.frame(DictionaryGI)

neg_words <- sa_df$negative
neg_words <- as.data.frame(neg_words)
neg_words <- neg_words %>% 
  mutate(sentiment = "negative")
names(neg_words) <- c('text','sentiment')

pos_words <- sa_df$positive
pos_words <- as.data.frame(pos_words)
pos_words <- pos_words %>% 
  mutate(sentiment="positive")
names(pos_words) <- c('text','sentiment')

sa_dict <- bind_rows(pos_words, neg_words) 

friends_sentiment <- friends_words %>%
  inner_join(sa_dict) %>%
  group_by(speaker,sentiment) %>% 
  summarise(total_count=n(),.groups = 'drop') %>%
  as.data.frame() %>%
  pivot_wider(names_from = sentiment,
              values_from = total_count)

colnames(friends_sentiment) <- c("speaker","negative_words","positive_words")
```
Showing count of positive and negative words used by Friends characters.
```{r}
friends_sentiment
```

Merging total Friends word count df with positive and negative word count df to assess % of all words which were positive or negative by character.
```{r}
friends_combined <- friends_summary %>%
  inner_join(friends_sentiment) %>%
  transform(perc_neg = scales::percent(negative_words / total_word_count),
            perc_pos = scales::percent(positive_words / total_word_count),
            neutral_words = total_word_count - negative_words - positive_words)

friends_combined <- friends_combined %>%
  transform(perc_neutr = scales::percent(neutral_words / total_word_count))

friends_final <- friends_combined[,c('speaker','total_word_count','speaking_lines','words_per_line','positive_words','negative_words','neutral_words','perc_pos','perc_neg','perc_neutr')]
```
In general, this dataframe shows the characters with a relatively narrow band of sentiment: from 4.00% positive (Ross) to 4.43% positive (Phoebe) and from 2.43% negative (Ross) to 2.78% negative (Monica). Neutrality ranged from 93.04% (Rachel) to 93.56% (Ross).

These data show Rachel as the most sentimental character, Ross as the least sentimental, Phoebe as the most positive, and Monica as the most negative. All of this aligns with my domain knowledge, therefore the sentiment analysis appears successful.
```{r}
friends_final
```
